{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Robust Controlled Trials\n",
    "\n",
    "The Randomized Controlled Trial (RCT) is a trusted method in experimental design that aims to figure out responses to certain interventions, while reducing the discrepancy in results due to variance in subjects. In fact, in 2019, Prof. Duflo and Banerjee from MIT got the Nobel Prize in Economics for addressing questions/issues in development economics (esp. poverty and availability of healthcare) using RCTs. \n",
    "\n",
    "But very few people discuss the fact that RCTs are quite ineffective in one major aspect: They rely on the **Central Limit Theorem** to ensure that control and experiments have similar inter-group and intra-group distributions of traits. This requires large experimental populations, which are expensive and often unavailable. \n",
    "\n",
    "Instead, there is research to suggest that **optimal controlled trials** can be significantly more powerful. \n",
    "\n",
    "This demo will hopefully demonstrate that randomization is NOT a reliable method for getting the right distribution of \"features\" in subjects. Furthermore, it will demonstrate the influence of robustness to uncertainty in our subject data on optimal experimental design. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivational Problem: Medical Trials\n",
    "\n",
    "Suppose that we only have the budget to conduct initial Covid-19 vaccine trials on 20 patients, where the patients are split 50/50 between control and treatment groups. We have had 30 applicants with 5 traits, generated randomly in this instance. (We have chosen small numbers since this problem can quickly become computationally challenging, but it is definitely solvable in larger scale as well.)\n",
    "\n",
    "First, we initialize our computational environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating Julia environment\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "using JuMP, Distributions, Random, LinearAlgebra, Gurobi, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we generate some random patients, with traits being sampled from the standard normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_random_people(n_people::Int64 = 20, n_traits::Int64 = 5, seed = 314)\n",
    "    # NOTE THAT OUR DATA IS NORMALIZED, so it makes the formulation more straight-forward. \n",
    "    continuous_values = rand(MersenneTwister(seed), Normal(0.00, 1), (n_people, n_traits))\n",
    "    return continuous_values\n",
    "end\n",
    "n_groups = 2\n",
    "n_patients = 20\n",
    "n_ppg = Int(n_patients/n_groups)\n",
    "n_people = 25\n",
    "n_traits = 5\n",
    "data = generate_random_people(n_people, n_traits)\n",
    "target_means = vec(sum(data[:, :], dims=1)/n_people)\n",
    "target_variances = vec(sum(data[:, :].^2, dims=1)/n_people);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Function to compute the elementwise maximum. Will be useful later! \"\"\"\n",
    "function elem_maximum(a...)\n",
    "    na = []\n",
    "    for i = 1:length(a[1])\n",
    "        push!(na, maximum([elem[i] for elem in a]))\n",
    "    end\n",
    "    return na\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initially, we will create our control and experiment groups by simply picking the first 10 to be in the control, and the next 10 to be in the experiment groups. Then, we can evaluate the means and variances of the traits of patients in each group, and compare them. \n",
    "(For simplicity, we will only consider the diagonal of the covariance matrix. However, this method can be extended to the full covariance matrix, by adding a lot more variables!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Experimental design through randomization\n",
    "ctrl_idxs = Int64.(collect(1:n_patients/2))\n",
    "vacc_idxs = Int64.(collect(n_patients/2+1:n_patients))\n",
    "function print_details(data, ctrl_idxs, vacc_idxs)\n",
    "    println(\"Control group: \", ctrl_idxs)\n",
    "    println(\"Vaccine group: \", vacc_idxs)\n",
    "    println(\"Mean traits of control group: \", round.(mean(data[ctrl_idxs, :], dims=1); sigdigits = 4))\n",
    "    println(\"Mean traits of vaccine group: \", round.(mean(data[vacc_idxs, :], dims=1); sigdigits = 4))\n",
    "    println(\"Var of traits of control group: \", round.(var(data[ctrl_idxs, :], dims=1); sigdigits = 4))\n",
    "    println(\"Var of traits of vaccine group: \", round.(var(data[vacc_idxs, :], dims=1); sigdigits = 4))\n",
    "    mean_errors = abs.(mean(data[ctrl_idxs, :], dims=1)' - mean(data[vacc_idxs, :], dims=1)')\n",
    "    println(\"Nominal objective: \", round(\n",
    "        sum(elem_maximum(abs.(mean(data[ctrl_idxs, :], dims=1)' .- mean(data[vacc_idxs, :], dims=1)'),\n",
    "                         abs.(mean(data[vacc_idxs, :], dims=1)' .- target_means),\n",
    "                         abs.(mean(data[ctrl_idxs, :], dims=1)' .- target_means))) + \n",
    "        0.5*sum(elem_maximum(abs.(var(data[ctrl_idxs, :], dims=1)' .- var(data[vacc_idxs, :], dims=1)'),\n",
    "                             abs.(var(data[ctrl_idxs, :], dims=1)' .- target_variances),\n",
    "                             abs.(var(data[vacc_idxs, :], dims=1)' .- target_variances))); sigdigits = 4))\n",
    "    return\n",
    "end\n",
    "print_details(data, ctrl_idxs, vacc_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will describe the objective function in greater detail later, but we can think about it as some sum of errors between the means and variances of the experiment and control groups. \n",
    "\n",
    "We can examine these errors more specifically by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function plot_errors(data, ctrl_idxs, vacc_idxs)\n",
    "    p1 = bar(1:5, vec(sum(data[ctrl_idxs, :], dims=1)/n_ppg .-  sum(data[vacc_idxs, :], dims=1)/n_ppg))\n",
    "    xlabel!(\"Features\")\n",
    "    ylabel!(\"Intra-group mean error\")\n",
    "    ylims!((-0.8, 0.8))\n",
    "    p2 = bar(1:5, vec(sum(data[ctrl_idxs, :], dims=1)/n_ppg) - vec(sum(data[:, :], dims=1)/n_people))\n",
    "    xlabel!(\"Features\")\n",
    "    ylabel!(\"Control group mean error w.r.t. population\")\n",
    "    ylims!((-0.8, 0.8))\n",
    "    p3 = bar(1:5, vec(sum(data[vacc_idxs, :], dims=1)/n_ppg) - vec(sum(data[:, :], dims=1)/n_people))\n",
    "    xlabel!(\"Features\")\n",
    "    ylabel!(\"Experiment group mean error w.r.t. population\")\n",
    "    ylims!((-0.8, 0.8))\n",
    "    plot(p1, p2, p3, layout = (1,3), legend = false)\n",
    "end\n",
    "plot_errors(data, ctrl_idxs, vacc_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, there are some discrepancies between the means and variances of the groups.\n",
    "\n",
    "### Can Optimization do better? \n",
    "It sure can! In this case, we will pick 2 groups of equal numbers of patients from the population, while minimizing some notion of error between the moments of traits in the two populations. \n",
    "\n",
    "To begin, we are going to define some target means $\\bar{\\mu}_j$ and variances $\\bar{\\sigma}_j$. For this toy example, we are going to assume that our target mean is the whole population mean, and the target variance is the whole population variance. (In theory, we can aim to achieve any target mean and variance we would like, for any number of groups.)\n",
    "\n",
    "Then we are going to define new variables $M_j$ and $V_j$, where $j$ is the trait index. These will define the maximum of inter-population and intra-group mean and variance errors as follows. \n",
    "\n",
    "$M_j = \\rm{max}\\Big(|\\mu_{j,1} - \\mu_{j,2}|, |\\mu_{j,1} - \\bar{\\mu}_j|, |\\mu_{j,2} - \\bar{\\mu}_j||\\Big),$\n",
    "\n",
    "$V_j = \\rm{max}\\Big(|\\sigma_{j,1} - \\sigma_{j,2}|, |\\sigma_{j,1} - \\bar{\\sigma}_j|, |\\sigma_{j,2} - \\bar{\\sigma}_j||\\Big).$\n",
    "\n",
    "The **intra-group** error is the differences in the values between the experiment and control groups. The **inter-population** error is the difference between the values between each group and the population as a whole. \n",
    "\n",
    "Our objective function will be a mixture of these two errors:\n",
    "\n",
    "Objective function $= \\sum M_j + 0.5\\sum V_j$\n",
    "\n",
    "Note that we are not limited to this objective function. (For example, we could try minimizing variance while keeping the mean variation below a threshold... we can try any combination that is bounded!)\n",
    "\n",
    "This way, the groups will look like each other and like the entire population as much as possible. Let's actually solve this representation of the problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's start creating out model, and trying to solve without uncertainty\n",
    "m = Model(Gurobi.Optimizer)\n",
    "# set_optimizer_attribute(m, \"tm_lim\", 60 * 1_000)\n",
    "# set_optimizer_attribute(m, \"msg_lev\", GLPK.GLP_MSG_ON)\n",
    "@variable(m, x[i=1:n_people, 1:n_groups], Bin)\n",
    "@variable(m, μ_p[i=1:n_groups, j=1:n_traits]) # Mean\n",
    "@variable(m, σ_p[i=1:n_groups, j=1:n_traits]) # Variance\n",
    "for j = 1:n_groups # Taking the mean and std deviation of parameters for each group\n",
    "    @constraint(m, μ_p[j,:] .== 1/(n_ppg) * \n",
    "                    sum(data[i,:] .* x[i,j] for i=1:n_people))\n",
    "    @constraint(m, σ_p[j,:] .== 1/(n_ppg) * \n",
    "                    sum((data[i,:] - target_means).^2 .* x[i,j] for i=1:n_people)) # Note the approximation here...\n",
    "    @constraint(m, sum(x[:,j]) == n_ppg)\n",
    "end\n",
    "for i = 1:n_people\n",
    "    @constraint(m, sum(x[i, :]) <= 1) # each patient only picked at most once\n",
    "end\n",
    "\n",
    "@variable(m, d)\n",
    "@variable(m, M[1:n_traits]) # mean error\n",
    "@variable(m, V[1:n_traits]) # variance error\n",
    "rho = 0.5\n",
    "@objective(m, Min, sum(M) + rho*sum(V))\n",
    "for i = 1:n_groups\n",
    "    @constraint(m, M[:] .>= μ_p[i,:] - target_means)\n",
    "    @constraint(m, M[:] .>= -(μ_p[i,:] - target_means))\n",
    "    @constraint(m, V[:] .>= σ_p[i, :] - target_variances)\n",
    "    @constraint(m, V[:] .>= -(σ_p[i, :] - target_variances))\n",
    "end\n",
    "for i = 1:n_groups-1\n",
    "    for j = i+1:n_groups\n",
    "        @constraint(m, M[:] .>= μ_p[i, :] - μ_p[j,:])\n",
    "        @constraint(m, M[:] .>= -(μ_p[i, :] - μ_p[j,:]))\n",
    "        @constraint(m, V[:] .>= σ_p[i, :] - σ_p[j,:])\n",
    "        @constraint(m, V[:] .>= -(σ_p[i, :] - σ_p[j,:]))\n",
    "    end\n",
    "end\n",
    "optimize!(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the results, and how our patients (by index) have changed compared to randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Results\n",
    "ctrl_opt = findall(x -> x >= 0.8, Array(value.(x[:,1])))\n",
    "vacc_opt = findall(x -> x >= 0.8, Array(value.(x[:,2])))\n",
    "print_details(data, ctrl_opt, vacc_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a massive improvement in the objective function! We can also see the reduction in mean and variance errors graphically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution of errors\n",
    "plot_errors(data, ctrl_opt, vacc_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What this means is that optimization dramatically improves on the quality of the experiment and control groups generated via randomization. It can thus improve the ability of experiments to have significant results, and can allow us to reduce the number of subjects we require to obtain that significance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Robust Optimization (RO) change our solutions? \n",
    "It is very possible that there is some error in the values of the traits of each subject. These errors could arise from a variety of sources: measurement error, observer's paradox, or even due to subjects lying in order to influence their perceived likelihood of being in the treatment group!\n",
    "\n",
    "We will deal with this kind of uncertainty through **robust OCTs**. Robust OCT will assume that there is uncertainty in the data in the problem that is defined by an uncertainty set. One potential description of the uncertainty is a **budget uncertainty set**, where the maximum absolute deviation of each trait for each subject is constrained ($||\\mathbf{z}||_{\\infty} \\leq \\rho$), as well as the cumulative absolute deviations of each trait for all subjects ($||\\mathbf{z}||_{1} \\leq \\Gamma$). In this case, we assume such an uncertainty set, with an maximum absolute deviation $\\rho = 0.1$, and a cumulative deviation $\\Gamma = 1$. Intuitively, this would allow up to 10 trait perturbations of magnitude 0.1, or 20 perturbations of magnitude 0.05, and so on. \n",
    "\n",
    "We start our optimization model similarly, by defining the means and variances, as well as the objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start creating out model, and trying to solve with uncertainty\n",
    "rm = Model(Gurobi.Optimizer)\n",
    "# set_optimizer_attribute(m, \"tm_lim\", 60 * 1_000)\n",
    "# set_optimizer_attribute(m, \"msg_lev\", GLPK.GLP_MSG_ON)\n",
    "@variable(rm, x[i=1:n_people, 1:n_groups], Bin)\n",
    "@variable(rm, μ_p[i=1:n_groups, j=1:n_traits]) # Mean\n",
    "@variable(rm, σ_p[i=1:n_groups, j=1:n_traits]) # Variance\n",
    "ρ = 0.1\n",
    "Γ = 1\n",
    "for j = 1:n_groups # Taking the mean and std deviation of parameters for each group\n",
    "    @constraint(rm, μ_p[j,:] .== 1/n_ppg * \n",
    "                    sum(data[i,:].*x[i,j] for i=1:n_people))\n",
    "    @constraint(rm, σ_p[j,:] .== 1/n_ppg * \n",
    "                    sum((data[i,:] - target_means).^2 .* x[i,j] for i=1:n_people)) # Note the approximation here...\n",
    "    @constraint(rm, sum(x[:,j]) == n_ppg)\n",
    "end\n",
    "for i = 1:n_people\n",
    "    @constraint(rm, sum(x[i, :]) <= 1)\n",
    "end\n",
    "@variable(rm, M[1:n_traits])\n",
    "@variable(rm, V[1:n_traits])\n",
    "@objective(rm, Min, sum(M) + 0.5*sum(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Note that we have defined some variables $\\mathbf{M}$ and $\\mathbf{V}$ to describe the errors. We cannot directly embed the uncertainty into means $\\mu$ and variances $\\sigma$, because these are defined by linear inequalities. Adding robustness to equalities would make this problem infeasible. Instead, we will minimize the **worst-case** errors as described by $\\mathbf{M}$ and $\\mathbf{V}$, by embedding the budget uncertainty set into the model through its robust counterpart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# With the following budget uncertainty\n",
    "# @uncertain(rm, ell[1:n_people, 1:n_traits])\n",
    "# @constraint(rm, norm(ell, 1) <= Γ)\n",
    "# @constraint(rm, -ρ .<= ell .<= ρ)  \n",
    "# Let's use the robust counterpart\n",
    "# FIRST, THE ROBUST COUNTERPART FOR THE INTRA-GROUP ERRORS.\n",
    "for g1 = 1:n_groups-1\n",
    "    for g2 = g1+1:n_groups\n",
    "        for l = 1:n_traits\n",
    "            y = @variable(rm, [1:n_people])\n",
    "            normdummy = @variable(rm, [1:n_people])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= (x[i,g1] - x[i,g2] - y[i]))\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= -(x[i,g1] - x[i,g2] - y[i]))\n",
    "            @constraint(rm, M[l] * n_ppg >= \n",
    "                        sum(data[k,l] .* (x[k,g1] - x[k,g2]) for k=1:n_people) + \n",
    "                        ρ * sum(normdummy) + Γ*infdummy)\n",
    "            y = @variable(rm, [1:n_people])\n",
    "            normdummy = @variable(rm, [1:n_people])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= (x[i,g1] - x[i,g2] - y[i]))\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= -(x[i,g1] - x[i,g2] - y[i]))\n",
    "            @constraint(rm, M[l] * n_ppg >= \n",
    "                        -(sum(data[k,l] .* (x[k,g1] - x[k,g2]) for k=1:n_people)) + \n",
    "                        ρ * sum(normdummy) + Γ*infdummy)\n",
    "            # Sometimes you have to get creative... linearization of the change of the variance. \n",
    "            y = @variable(rm, [1:n_people])\n",
    "            normdummy = @variable(rm, [1:n_people])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= (2*(data[i,l] - target_means[l])*(x[i,g1] - x[i,g2]) - y[i]))\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= -(2*(data[i,l] - target_means[l])*(x[i,g1] - x[i,g2]) - y[i]))\n",
    "            @constraint(rm, V[l] * n_ppg >= \n",
    "                        sum((data[k,l] - target_means[l]).^2 .* (x[k,g1] - x[k,g2]) for k=1:n_people) + \n",
    "                        ρ*sum(normdummy) + Γ*infdummy)\n",
    "            y = @variable(rm, [1:n_people])\n",
    "            normdummy = @variable(rm, [1:n_people])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= (2*(data[i,l] - target_means[l])*(x[i,g1] - x[i,g2]) - y[i]))\n",
    "            @constraint(rm, [i = 1:n_people], infdummy >= -(2*(data[i,l] - target_means[l])*(x[i,g1] - x[i,g2]) - y[i]))\n",
    "            @constraint(rm, V[l] * n_ppg >= \n",
    "                        -(sum((data[k,l] - target_means[l]).^2 .* (x[k,g1] - x[k,g2]) for k=1:n_people)) + \n",
    "                        ρ*sum(normdummy) + Γ*infdummy)\n",
    "        end\n",
    "    end\n",
    "end\n",
    "# THEN, THE ROBUST COUNTERPART FOR THE INTER_POPULATION ERRORS\n",
    "for g1 = 1:n_groups\n",
    "    for l = 1:n_traits\n",
    "        y = @variable(rm, [1:n_people])\n",
    "        normdummy = @variable(rm, [1:n_people])\n",
    "        @constraint(rm, normdummy .>= y)\n",
    "        @constraint(rm, normdummy .>= -y)\n",
    "        infdummy = @variable(rm)\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= (x[i,g1] - y[i]))\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= -(x[i,g1] - y[i]))\n",
    "        @constraint(rm, M[l] * n_ppg >= \n",
    "                    sum(data[k,l] .* x[k,g1] for k=1:n_people) - target_means[l] * n_ppg +\n",
    "                    ρ * sum(normdummy) + Γ*infdummy)\n",
    "        y = @variable(rm, [1:n_people])\n",
    "        normdummy = @variable(rm, [1:n_people])\n",
    "        @constraint(rm, normdummy .>= y)\n",
    "        @constraint(rm, normdummy .>= -y)\n",
    "        infdummy = @variable(rm)\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= (x[i,g1] - y[i]))\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= -(x[i,g1] - y[i]))\n",
    "        @constraint(rm, M[l] * n_ppg >= \n",
    "                    -(sum(data[k,l] .* x[k,g1] for k=1:n_people) - target_means[l] * n_ppg) +\n",
    "                    ρ * sum(normdummy) + Γ*infdummy)\n",
    "        # Now for the variance\n",
    "        y = @variable(rm, [1:n_people])\n",
    "        normdummy = @variable(rm, [1:n_people])\n",
    "        @constraint(rm, normdummy .>= y)\n",
    "        @constraint(rm, normdummy .>= -y)\n",
    "        infdummy = @variable(rm)\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= (2*(data[i,l] - target_means[l])*(x[i,g1]) - y[i]))\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= -(2*(data[i,l] - target_means[l])*(x[i,g1]) - y[i]))\n",
    "        @constraint(rm, V[l] * n_ppg >= \n",
    "                    sum((data[k,l] - target_means[l]).^2 .* x[k,g1] for k=1:n_people) - target_variances[l] * n_ppg + \n",
    "                    ρ*sum(normdummy) + Γ*infdummy)\n",
    "        y = @variable(rm, [1:n_people])\n",
    "        normdummy = @variable(rm, [1:n_people])\n",
    "        @constraint(rm, normdummy .>= y)\n",
    "        @constraint(rm, normdummy .>= -y)\n",
    "        infdummy = @variable(rm)\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= (2*(data[i,l] - target_means[l])*(x[i,g1]) - y[i]))\n",
    "        @constraint(rm, [i = 1:n_people], infdummy >= -(2*(data[i,l] - target_means[l])*(x[i,g1]) - y[i]))\n",
    "        @constraint(rm, V[l] * n_ppg >= \n",
    "                    -(sum((data[k,l] - target_means[l]).^2 .* x[k,g1] for k=1:n_people) - target_variances[l] * n_ppg) + \n",
    "                    ρ*sum(normdummy) + Γ*infdummy)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "optimize!(rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like we have found a design of experiments that is robust to the budget uncertainty set! Let's take a look at the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Robust optimal results:\")\n",
    "ctrl_ro = findall(x -> x >= 0.8, Array(value.(x[:,1])))\n",
    "vacc_ro = findall(x -> x >= 0.8, Array(value.(x[:,2])))\n",
    "print_details(data, ctrl_ro, vacc_ro)\n",
    "println(\"Robust worst-case objective: \", objective_value(rm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(\"Deterministic optimal results, for comparison:\")\n",
    "print_details(data, ctrl_opt, vacc_opt)\n",
    "println(\"\")\n",
    "println(\"Random selection results, for comparison:\")\n",
    "print_details(data, ctrl_idxs, vacc_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe slight changes in the optimal allocations of patients. Obviously, the nominal objective is strictly worsened under robustness considerations. However, in presence of uncertainty, this new experimental design makes sure that the worst case outcome of our objective function is only around x\\% worse than the nominal case. The OED without robustness is a lot more sensitive to this uncertainty. Below, we compute the worst-case objective of the random, optimal and robust cases for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal worst case\n",
    "set_optimizer_attribute(rm, \"OutputFlag\", 0)\n",
    "constrs = [@constraint(rm, x[ctrl_idxs,1] .== ones(n_ppg))..., @constraint(rm, x[vacc_idxs, 2] .== ones(n_ppg))...]\n",
    "optimize!(rm)\n",
    "nom_worst = objective_value(rm);\n",
    "println(\"Random worst-case: \" * string(nom_worst))\n",
    "delete.(rm, constrs);\n",
    "constrs = [@constraint(rm, x[ctrl_opt,1] .== ones(n_ppg))..., @constraint(rm, x[vacc_opt, 2] .== ones(n_ppg))...]\n",
    "optimize!(rm)\n",
    "opt_worst = objective_value(rm)\n",
    "println(\"Optimal worst-case: \" * string(opt_worst))\n",
    "delete.(rm, constrs);\n",
    "constrs = [@constraint(rm, x[ctrl_ro,1] .== ones(n_ppg))..., @constraint(rm, x[vacc_ro, 2] .== ones(n_ppg))...]\n",
    "optimize!(rm)\n",
    "ro_worst = objective_value(rm)\n",
    "println(\"Robust optimal worst-case: \" * string(ro_worst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we see is that the robust optimal solution definitively has the lowest worst-case objective, and while the optimal solution (without uncertainty) has a better nominal outcome, it is a lot more sensitive to uncertainties in the traits of the subjects. The randomized cases is again by far the worst, having the worst performance both without and with uncertainty. \n",
    "\n",
    "For curiosity's sake, we also plot the mean and variance errors for the robust solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(data, ctrl_ro, vacc_ro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future work\n",
    "\n",
    "One thing to improve immediately is:\n",
    "- The variance error approximation! We need to deal with the bilinearity to have much better results in the margins. \n",
    "\n",
    "This kind of experimental design can be dramatically improved by: \n",
    "- Adding consideration for categorical variables,\n",
    "- Considering different types of uncertainty in the features. These could be: \n",
    "    - Column- (i.e. feature-) wise, or\n",
    "    - Row- (i.e. patient-) wise. \n",
    "- Constructing uncertainty sets directly from uncertainty in data, and \n",
    "- Adding the ability to simulate group performance under different uncertain outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Optimal experimental design and OCTs is a useful method to make sure that the moments of the features our experiment and control groups are similar, while adhering to a target distribution of features. \n",
    "- Uncertainty can result from a variety of factors in experimental designs. \n",
    "- Robust OCTs can improve the efficacy of experiments with small reduction in nominal performance compared to optimized groups without uncertainty. However, robust OCT gives significantly better worst-case outcomes, and reduces the sensitivity of experiments to uncertainty in the traits of control and experiment subjects!\n",
    "- Randomization is consistently the **worst** method for experimental design by far!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.3",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
