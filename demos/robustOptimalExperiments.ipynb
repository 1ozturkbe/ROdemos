{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "incoming-approval",
   "metadata": {},
   "source": [
    "# Robust Optimal Experiments\n",
    "\n",
    "The Randomized Controlled Trial (RCT) is a trusted method in experimental design that aims to figure out responses to certain interventions, while reducing the discrepancy in results due to variance in subjects. In fact, in 2019, Prof. Duflo and Banerjee from MIT got the Nobel Prize in Economics for addressing questions/issues in development economics (esp. poverty and availability of healthcare) using RCTs. \n",
    "\n",
    "But very few people talked about the fact that RCTs are quite ineffective in one major aspect: They rely on the Central Limit Theorem to ensure that control and experiments have similar distributions of traits. This requires large experimental populations, which are expensive and often unavailable. \n",
    "\n",
    "Instead, there is research to suggest that **optimal experimental design (OED)** can be significantly more powerful. \n",
    "\n",
    "This demo will hopefully demonstrate that randomization is NOT a reliable method for getting the right distribution of \"features\" in subjects. Furthermore, it will demonstrate the influence of robustness on optimal experimental design. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-jerusalem",
   "metadata": {},
   "source": [
    "## Motivational Problem: Medical Trials\n",
    "\n",
    "Suppose that we only have the budget to conduct initial Covid-19 vaccine trials on 10 patients, where the patients are split 50/50 between control and treatment groups. We have had 20 applicants with 5 traits, generated randomly in this instance. (We have chosen small numbers since this problem can quickly become computationally challenging. But it is definitely solvable in larger scale as well.) \n",
    "\n",
    "First, we initialize our computational environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outstanding-compensation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activating Julia environment\n",
    "using Pkg\n",
    "Pkg.activate(\".\")\n",
    "using JuMP, Distributions, Random, LinearAlgebra, Gurobi, Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2a424b",
   "metadata": {},
   "source": [
    "Then, we generate some random patients, with traits being sampled from the standard normal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stable-handy",
   "metadata": {},
   "outputs": [],
   "source": [
    "function generate_random_people(n_people::Int64 = 20, n_traits::Int64 = 5)\n",
    "    # NOTE THAT OUR DATA IS NORMALIZED, so it makes the formulation more straight-forward. \n",
    "    continuous_values = rand(MersenneTwister(314), Normal(0.00, 1), (n_people, n_traits))\n",
    "    return continuous_values\n",
    "end\n",
    "n_groups = 2\n",
    "n_patients = 10\n",
    "n_people = 20\n",
    "n_traits = 5\n",
    "data = generate_random_people(20, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab088dad",
   "metadata": {},
   "source": [
    "Initially, we will create our control and experiment groups by simply picking the first 5 to be in the control, and the next 5 to be in the experiment groups. Then, we can evaluate the means and variances of the traits of patients in each group, and compare then. \n",
    "(For simplicity, we will only consider the diagonal of the covariance matrix. However, this method can be extended to the full covariance matrix, by adding a lot more variables!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "material-english",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental design through randomization\n",
    "ctrl_idxs = Int64.(collect(1:n_patients/2))\n",
    "vacc_idxs = Int64.(collect(n_patients/2+1:n_patients))\n",
    "function print_details(data, ctrl_idxs, vacc_idxs)\n",
    "    println(\"Control group: \", ctrl_idxs)\n",
    "    println(\"Vaccine group: \", vacc_idxs)\n",
    "    println(\"Mean traits of control group: \", round.(mean(data[ctrl_idxs, :], dims=1); sigdigits = 4))\n",
    "    println(\"Mean traits of vaccine group: \", round.(mean(data[vacc_idxs, :], dims=1); sigdigits = 4))\n",
    "    println(\"Var of traits of control group: \", round.(var(data[ctrl_idxs, :], dims=1); sigdigits = 4))\n",
    "    println(\"Var of traits of vaccine group: \", round.(var(data[vacc_idxs, :], dims=1); sigdigits = 4))\n",
    "    println(\"Nominal objective: \", round.(sum(abs.(mean(data[ctrl_idxs, :], dims=1) - mean(data[vacc_idxs, :], dims=1))) + \n",
    "            0.5 * sum(abs.(var(data[ctrl_idxs, :], dims=1) - var(data[vacc_idxs, :], dims=1))); sigdigits = 4))\n",
    "    return\n",
    "end\n",
    "print_details(data, ctrl_idxs, vacc_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e0b68a",
   "metadata": {},
   "source": [
    "Our objective function in this case is to minimize the absolute difference of the sum of means, and 1/2 times the absolute different of the sum of variances of traits between the control and experiment group. Note that we are not limited to this objective function; feel free to experiment in your own time with others. (For example, we could try minimizing variance while keeping the mean variation below a threshold... you can try any combination that is bounded!)\n",
    "\n",
    "We can examine the errors more specifically by plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acquired-toner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the errors\n",
    "function plot_errors(data, ctrl_idxs, vacc_idxs)\n",
    "    l = @layout [a ; b]\n",
    "    p1 = bar(collect(1:n_traits), mean(data[ctrl_idxs, :], dims=1)' .-  mean(data[vacc_idxs, :], dims=1)', label = \"Mean errors\")\n",
    "    p2 = bar(collect(1:n_traits), var(data[ctrl_idxs, :], dims=1)' .- var(data[vacc_idxs, :], dims=1)', label = \"Variance errors\")\n",
    "    plt = plot(p1, p2, layout = l)\n",
    "    return plt\n",
    "end\n",
    "plt = plot_errors(data, ctrl_idxs, vacc_idxs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upper-explorer",
   "metadata": {},
   "source": [
    "Clearly, there are some discrepancies between the means and variances of the groups.\n",
    "\n",
    "### Can Optimization do better? \n",
    "It sure can! Let's start by writing out the problem in a mixed-integer optimization format. \n",
    "\n",
    "In this case, we will pick 2 groups of equal numbers of patients from the population, while minimizing the weighted L1-norm error in the mean and variances between the two groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adopted-aruba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start creating out model, and trying to solve without uncertainty\n",
    "m = Model(Gurobi.Optimizer)\n",
    "set_optimizer_attribute(m, \"OutputFlag\", 0)\n",
    "@variable(m, x[i=1:n_people, 1:n_groups], Bin)\n",
    "@variable(m, μ_p[i=1:n_groups, j=1:n_traits]) # Mean\n",
    "@variable(m, σ_p[i=1:n_groups, j=1:n_traits]) # Variance\n",
    "for j = 1:n_groups # Taking the mean and std deviation of parameters for each group\n",
    "    @constraint(m, μ_p[j,:] .== 1/(n_patients/n_groups) * \n",
    "                    sum(data[i,:] .* x[i,j] for i=1:n_people))\n",
    "    @constraint(m, σ_p[j,:] .== 1/(n_patients/n_groups) * \n",
    "                    sum(data[i,:].^2 .* x[i,j] for i=1:n_people))\n",
    "    @constraint(m, sum(x[:,j]) == n_patients/n_groups)\n",
    "end\n",
    "for i = 1:n_people\n",
    "    @constraint(m, sum(x[i, :]) <= 1) # each patient only picked at most once\n",
    "end\n",
    "\n",
    "@variable(m, d)\n",
    "@variable(m, M[1:n_traits]) # mean error\n",
    "@variable(m, V[1:n_traits]) # variance error\n",
    "rho = 0.5\n",
    "@objective(m, Min, sum(M) + rho*sum(V))\n",
    "for i = 1:n_groups\n",
    "    for j = i+1:n_groups\n",
    "        @constraint(m, M[:] .>= μ_p[i,:] - μ_p[j,:])\n",
    "        @constraint(m, M[:] .>= μ_p[j,:] - μ_p[i,:])\n",
    "        @constraint(m, V[:] .>= σ_p[i, :] - σ_p[j, :])\n",
    "        @constraint(m, V[:] .>= σ_p[j, :] - σ_p[i, :])\n",
    "    end\n",
    "end\n",
    "optimize!(m)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384651da",
   "metadata": {},
   "source": [
    "Let's see the results, and how our patients (by index) have changed compared to randomization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "optical-obligation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results\n",
    "ctrl_opt = findall(x -> x == 1, Array(value.(x[:,1])))\n",
    "vacc_opt = findall(x -> x == 1, Array(value.(x[:,2])))\n",
    "print_details(data, ctrl_opt, vacc_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1b7138",
   "metadata": {},
   "source": [
    "We see a more than 10x improvement in the objective function! We can also see the reduction in mean and variance errors graphically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e6f0ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the distribution\n",
    "plot_errors(data, ctrl_opt, vacc_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-france",
   "metadata": {},
   "source": [
    "### How does Robust Optimization (RO) change our solutions? \n",
    "It is very possible that there is some error in the values of the traits of each subject. These errors could arise from a variety of sources: measurement error, observer's paradox, or even due to subjects lying in order to influence their perceived likelihood of being in the treatment group!\n",
    "\n",
    "We will deal with this kind of uncertainty through **robust OED**. Robust OED will assume that there is uncertainty in the data in the problem that is defined by an uncertainty set. One potential description of the uncertainty is a **budget uncertainty set**, where the maximum absolute deviation of each trait for each subject is constrained ($||\\mathbf{z}||_{\\infty} \\leq \\rho$), as well as the cumulative absolute deviations of each trait for all subjects ($||\\mathbf{z}||_{1} \\leq \\Gamma$). In this case, we assume such an uncertainty set, with an maximum absolute deviation $\\rho = 1$, and a cumulative deviation $\\Gamma = 2$. Intuitively, this would allow up to 20 trait perturbations of magnitude 0.1, or 40 perturbations of magnitude 0.05, and so on. \n",
    "\n",
    "We start our optimization model similarly, by defining the means and variances, as well as the objective function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-confirmation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start creating out model, and trying to solve with uncertainty\n",
    "rm = Model(Gurobi.Optimizer)\n",
    "set_optimizer_attribute(rm, \"OutputFlag\", 0)\n",
    "@variable(rm, x[i=1:n_people, 1:n_groups], Bin)\n",
    "@variable(rm, μ_p[i=1:n_groups, j=1:n_traits]) # Mean\n",
    "@variable(rm, σ_p[i=1:n_groups, j=1:n_traits]) # Variance\n",
    "ρ = 0.1\n",
    "Γ = 2\n",
    "for j = 1:n_groups # Taking the mean and std deviation of parameters for each group\n",
    "    @constraint(rm, μ_p[j,:] .== 1/(n_patients/n_groups) * \n",
    "                    sum(data[i,:].*x[i,j] for i=1:n_people))\n",
    "    @constraint(rm, σ_p[j,:] .== 1/(n_patients/n_groups) * \n",
    "                    sum(data[i,:].^2 .* x[i,j] for i=1:n_people))\n",
    "    @constraint(rm, sum(x[:,j]) == n_patients/n_groups)\n",
    "end\n",
    "for i = 1:n_people\n",
    "    @constraint(rm, sum(x[i, :]) <= 1)\n",
    "end\n",
    "@variable(rm, M[1:n_traits])\n",
    "@variable(rm, V[1:n_traits])\n",
    "@objective(rm, Min, sum(M) + 0.5*sum(V))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e399c659",
   "metadata": {},
   "source": [
    "\n",
    "Note that we have defined some variables $\\mathbf{M}$ and $\\mathbf{V}$ to describe the errors. We cannot directly embed the uncertainty into means $\\mu$ and variances $\\sigma$, because these are defined by linear inequalities. Adding robustness to equalities would make this problem infeasible. Instead, we will minimize the **worst-case** errors as described by $\\mathbf{M}$ and $\\mathbf{V}$, by embedding the budget uncertainty set into the model through its robust counterpart. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9db2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the following budget uncertainty\n",
    "# @uncertain(rm, ell[1:n_people, 1:n_traits])\n",
    "# @constraint(rm, norm(ell, 1) <= Γ)\n",
    "# @constraint(rm, -ρ .<= ell .<= ρ)  \n",
    "# Let's use the robust counterpart\n",
    "for i = 1:n_groups # We embed the uncertainty in the errors!\n",
    "    for j = i+1:n_groups\n",
    "        for l = 1:n_traits\n",
    "            y = @variable(rm, [1:n_traits])\n",
    "            normdummy = @variable(rm, [1:n_traits])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= (x[k,j] - x[k,i] - y[k]))\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= -(x[k,j] - x[k,i] - y[k]))\n",
    "            @constraint(rm, M[l] * n_patients/n_groups >= \n",
    "                        sum(data[k,l] .* (x[k,j] - x[k,i]) for k=1:n_people) + \n",
    "                        ρ * sum(normdummy) + Γ*infdummy)\n",
    "            y = @variable(rm, [1:n_traits])\n",
    "            normdummy = @variable(rm, [1:n_traits])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= (x[k,j] - x[k,i] - y[k]))\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= -(x[k,j] - x[k,i] - y[k]))\n",
    "            @constraint(rm, M[l] * n_patients/n_groups >= \n",
    "                        - sum(data[k,l] .* (x[k,j] - x[k,i]) for k=1:n_people) +  \n",
    "                        ρ * sum(normdummy) + Γ*infdummy)\n",
    "#             Sometimes you have to get creative... linearization of the change of the variance. \n",
    "            y = @variable(rm, [1:n_traits])\n",
    "            normdummy = @variable(rm, [1:n_traits])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= (2*data[k,l]*(x[k,j] - x[k,i]) - y[k]))\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= -(2*data[k,l]*(x[k,j] - x[k,i]) - y[k]))\n",
    "            @constraint(rm, V[l] * n_patients/n_groups >= \n",
    "                        sum(data[k,l].^2 .* (x[k,j] - x[k,i]) for k=1:n_people) + \n",
    "                        ρ*sum(normdummy) + Γ*infdummy)\n",
    "            y = @variable(rm, [1:n_traits])\n",
    "            normdummy = @variable(rm, [1:n_traits])\n",
    "            @constraint(rm, normdummy .>= y)\n",
    "            @constraint(rm, normdummy .>= -y)\n",
    "            infdummy = @variable(rm)\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= (2*data[k,l]*(x[k,j] - x[k,i]) - y[k]))\n",
    "            @constraint(rm, [k = 1:n_traits], infdummy >= -(2*data[k,l]*(x[k,j] - x[k,i]) - y[k]))\n",
    "            @constraint(rm, V[l] * n_patients/n_groups >= \n",
    "                        -sum(data[k,l].^2 .* (x[k,j] - x[k,i]) for k=1:n_people) + \n",
    "                        ρ*sum(normdummy) + Γ*infdummy)\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "comfortable-lingerie",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize!(rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a06f3dd8",
   "metadata": {},
   "source": [
    "It seems like we have found a design of experiments that is robust to the budget uncertainty set! Let's take a look at the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intelligent-worry",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctrl_ro = findall(x -> x == 1, Array(value.(x[:,1])))\n",
    "vacc_ro = findall(x -> x == 1, Array(value.(x[:,2])))\n",
    "print_details(data, ctrl_ro, vacc_ro)\n",
    "println(\"Robust objective: \", objective_value(rm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c038fcc",
   "metadata": {},
   "source": [
    "We observe slight changes in the optimal allocations of patients. In addition, the nominal objective is worsened by around 80\\% under robustness considerations. However, in presence of uncertainty, this new experimental design makes sure that the worst case outcome of our objective function is only around 25\\% worse than the nominal case. The OED without robustness is a lot more sensitive to this uncertainty. Below, we compute the worst-case objective of the random, optimal and robust cases for comparison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominal worst case\n",
    "constrs = [@constraint(rm, x[ctrl_idxs,1] .== ones(5))..., @constraint(rm, x[vacc_idxs, 2] .== ones(5))...]\n",
    "optimize!(rm)\n",
    "nom_worst = objective_value(rm);\n",
    "println(\"Random worst-case: \" * string(nom_worst))\n",
    "delete.(rm, constrs);\n",
    "constrs = [@constraint(rm, x[ctrl_opt,1] .== ones(5))..., @constraint(rm, x[vacc_opt, 2] .== ones(5))...]\n",
    "optimize!(rm)\n",
    "opt_worst = objective_value(rm)\n",
    "println(\"Optimal worst-case: \" * string(opt_worst))\n",
    "delete.(rm, constrs);\n",
    "constrs = [@constraint(rm, x[ctrl_ro,1] .== ones(5))..., @constraint(rm, x[vacc_ro, 2] .== ones(5))...]\n",
    "optimize!(rm)\n",
    "ro_worst = objective_value(rm)\n",
    "println(\"Robust optimal worst-case: \" * string(ro_worst))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "831669a9",
   "metadata": {},
   "source": [
    "What we see is that the robust optimal solution definitively has the lowest worst-case objective, and while the optimal solution (without uncertainty) has a better nominal outcome, it is a lot more sensitive to uncertainties in the traits of the subjects. The randomized cases is again by far the worst, having the worst performance both without and with uncertainty. \n",
    "\n",
    "For curiosity's sake, we also plot the mean and variance errors for the robust solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aacf450",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_errors(data, ctrl_ro, vacc_ro)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-trail",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "- Optimal experimental design is a useful method to make sure that the moments of our experiment and control groups are similar, while still being representative of the global population. \n",
    "- Uncertainty can result from a variety of factors in experimental designs. \n",
    "- Robust optimal experimental design can improve the efficacy of experiments with small reduction in nominal performance compared to optimized groups without uncertainty. However, robust OED gives significantly better worst-case outcomes, and reduces the sensitivity of experiments to uncertainty in the traits of control and experiment subjects!\n",
    "- Randomization is consistently the **worst** method for experimental design by far!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.6.5",
   "language": "julia",
   "name": "julia-1.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
